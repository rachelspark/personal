---
title: Can LLMs be scientists?
description: An essay on LLMs and the philosophy of science
date: 'March 24, 2024'
---

*This is an essay I wrote for my tutorial at Oxford, during my quarter studying abroad in Winter 2024. I had a lot of fun grappling with this question, and thought it was pretty relevant to a lot of the innovations happening in AI research right now, hence why I'm publishing it here.*


Rapid progress in Artificial Intelligence (AI) in the past couple decades has begun to revolutionize scientific research, as seen with how scientists are leveraging deep learning models to discover new compounds, diagnose new diseases, and more. In fact, AI has advanced to a point that it now holds the promise of providing important concepts and methods itself, potentially driving forward science autonomously. At the frontier of AI are autoregressive large language models (LLMs), statistical natural language models that predict the next token in a sequence based on previous words in the sequence. These LLMs have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications, which has largely advanced due to developments in autoregressive network architecture such as the [attention mechanism](https://arxiv.org/abs/1706.03762). One can imagine how, given some actuator that reliably performs any instruction given in natural language, these LLMs may be able to automate the scientific process entirely. An LLM scientist would automatically form hypotheses, devise experiments to test these hypotheses, physically run the experiment using its actuator, then interpret the results. The question then arises of whether this is possible: can agents driven by autoregressive LLMs be scientists?
In order to answer this question, we must first dissect how humans construct new scientific theories, then discuss whether autoregressive LLMs can properly conduct these processes. In this paper, I argue that logical reasoning is crucial to the nature of scientific discovery, and that LLMs fall short of the reasoning capabilities required for making scientific discoveries. To construct this argument, I will first discuss the nature of developing new concepts in scientific discovery, and then the limitations of autoregressive LLMs in meeting these expectations.

## The logic of scientific discovery
The process of scientific inquiry appears to follow a logical and systematic structure, using observations and empirical evidence to reach conclusions closer to the ground truth. This structure can be illuminated through the concepts of abduction, induction, and deduction, a framework conceived by the philosopher and logician [Charles Sanders Peirce](https://plato.stanford.edu/entries/abduction/peirce.html) in the late nineteenth century that offers a comprehensive understanding of the inferential processes underlying the process of scientific discovery. Induction, the process of deriving generalized conclusions from specialized observations, and deduction, the process of drawing a specialized conclusion that follows from true premises, are familiar types of logical reasoning that can be distinguished by the latter’s basis in a ground truth. [Abduction](https://plato.stanford.edu/entries/abduction/), a term coined by Pierce, denotes a type of non-deductive inference distinct from induction that can best be described as inference to best explanation for a given set of observations. For Pierce, abductive reasoning is as crucial to the process of scientific discovery as inductive and deductive reasoning, and forms the basis of the first of the three phases of the scientific method: formulating explanatory hypotheses. Abduction is the logical operation which introduces any new idea for us to further test. Deduction and induction, then, play a later role when assessing these scientific theories: deduction helps us derive testable consequences from the explanatory hypotheses that abduction helped us come up with, and induction helps us reach a verdict on the hypotheses. 

We can illustrate these three phases of the scientific method, particularly the importance of abduction, with the case of the discovery of Neptune. At the beginning of the nineteenth century, it was discovered that Uranus, one of the only seven planets known at the time, had departed from the orbit predicted upon Isaac Newton’s theory of universal gravitation. One possible conclusion, one that follows Popperian falsification as we will discuss later in this paper, was that Newton’s theory is false. Yet astronomers John Couch Adams and Urbain Le Verrier surmised that given the success of Newton’s theory for at least two centuries by that time, this seemed unlikely, and hypothesized instead that there was an eighth, yet to be discovered planet. They then used Newton’s laws to attempt to calculate the position of the hypothetical planet and gathered observatories around the world looking for it. Not long after, the planet Neptune was found at the Royal Observatory by Johann Galle, with Galle writing to Le Verrier: “Monsieur, the planet of which you indicated the position really exists.” This anecdote shows the importance of abduction, as well as the interplay between all three different types of reasoning, in the process of scientific inquiry. Rather than interpreting the evidence contradictory to an existing theory as a means to immediately falsify that theory, Adams and Le Verrier used abductive reasoning to generate a new hypothesis that could explain the observed anomaly. Their hypothesis—that an unseen planet’s gravitational influence was affecting Uranus—offered the best explanation for the discrepancies observed. Given this hypothesis, deductive reasoning then enabled them to calculate where the new planet should be located in the universe using Newton’s laws, turning the abstract hypothesis into concrete, testable predictions. Inductive reasoning then came into play once telescopes pointed to the predicted part of the sky confirmed the existence of Neptune, corroborating their explanatory hypothesis with empirical evidence. This iterative cycle of abduction, deduction, and induction underscores the dynamic yet inherently logical nature of scientific inquiry, where the distinct patterns that underlie the formulation of hypotheses, generation of predictions, and empirical verification work in concert to advance our understanding of the world.

## Are LLMs able to reason?
Given that the process of scientific discovery is rooted in abductive, inductive, and deductive reasoning, we must evaluate whether LLMs have the capacity for such logical reasoning to determine whether they are capable of doing science. There are many indications that LLMs may be able to reason, given their [high performance across a variety of reasoning tasks](https://arxiv.org/abs/2210.09261). It has also been shown that in-context learning techniques such as [chain-of-thought prompting](https://arxiv.org/abs/2201.11903), in which the models are directly prompted with something along the lines of “Let’s think step by step,” enable LLMs to better answer questions with explicit reasoning steps such as arithmetic problems. Furthermore, [Dasgupta et al.](https://arxiv.org/abs/2207.07051) also found that LLMs exhibit reasoning patterns that are similar to those of humans as described in cognitive literature. However, these findings cannot conclusively establish that autoregressive LLMs can truly reason. For one, even though these LLMs seem to pass many of the existing benchmarks on reasoning, it is not clear whether the models are making their predictions based on reasoning or simple heuristics, since we are able to design a program with heuristic rules to achieve similarly high performance. Moreover, while techniques such as chain-of-thought prompting seem to elicit step-by-step reasoning in LLMs, the generated proofs are often incorrect and inconsistent. It is possible that models are generating “reasoning-like responses,” based on the patterns that they have been able to deduce in the natural language of their training data, rather than true step-by-step reasoning. In fact, [researchers have found](https://arxiv.org/abs/2210.01240) that even with chain-of-thought prompting, LLMs tend to choose the wrong steps when multiple options are available, leading to incomplete or incorrect proofs, and are able to produce proofs even when the synthetic ontology is fictional or counterfactual. One can imagine how this sense of “greedy reasoning” in LLMs may hinder the process of scientific inquiry, as LLM may prematurely falsify a proposed hypothesis, turn a hypothesis into baseless predictions, or come up with a persuasive proof for a claim without any empirical basis, perverting the steps of abduction, induction, and deduction, respectively. In the case of the discovery of Neptune, for example, there is potential that an LLM given evidence of the anomaly in Neptune’s orbit would use it to declare that Newton’s theory was no longer valid. Given that LLMs are trained on simply massive amounts of textual data, it is hard to expect an LLM to generate and reason through many plausible hypotheses, and understand which one is the best possible explanation as well as a truly novel and testable such as the one of an undiscovered planet in the universe. It is even harder to expect an LLM to consistently and accurately reason testable consequences of this hypothesis through deductive processes, given that the architecture of a large language model is entirely based on probabilities. [Researchers have shown](https://arxiv.org/abs/2202.07206) that the performance of LLMs on downstream tasks is sensitive to the frequency of certain terms, such as numbers, in the training data, which would be unexpected if the models were solving math problems through reasoning. It may thereby be ignorant to assume that an LLM would be able to reliably, for instance, apply Newton’s theory of gravitation and calculate the location of the undiscovered planet using the given evidence. Finally, just because LLMs exhibit some human-like reasoning patterns, as shown by Dasgupta et. al, that does not necessarily mean that they behave entirely like humans. Therefore, while it is undeniable that state-of-the-art LLMs exhibit the appearance of reasoning—by generating coherent and contextually relevant text that often seems logically intact—their high performance on our simple benchmarks does not necessarily mean that they are capable of the complex and robust reasoning required of human scientists. 

## Popperian falsification
Some may argue autoregressive LLMs can make scientific insights even with limited logical reasoning capabilities, using [Karl Popper](https://plato.stanford.edu/entries/popper/)’s theory of scientific discovery for support. Popper, one of the twentieth century’s most influential philosophers of science, believed there is no “logic” to the process of scientific discovery. In Popper’s view, scientific progress is driven by means of wild imaginative guesswork, controlled by aggressive empirical refutation. Based on this theory, LLMs may then be able to provide value in how they hallucinate, producing language that is coherent but factually incorrect or nonsensical. This would enable them to do the “imaginative guesswork” in generating falsifiable hypotheses, then do the more straightforward task of designing experiments that test these hypotheses. However, this argument breaks when taking into account that generating sound rationales and conclusions from these experiments requires deductive logic, and the limitations of LLMs continue to apply here. Moreover, Popper’s claim on the logic of scientific discovery is fundamentally flawed, in that the idea of falsification overlooks the notion of scientific inquiry as a nuanced, complex process that involves making educated predictions, reviewing previous assumptions, and intuiting general conclusions from observations, as done with abductive, deductive, and inductive reasoning. For example, it is easy to imagine how an LLM, with its limited reasoning capabilities, would take a single contradictory observation and immediately jump to the conclusion that a given hypothesis is false. This approach would appear to be proper science according to Popperian falsifiability, but would not contribute to the overall progress of our understanding of the world. Therefore although autoregressive LLMs may be able to conduct processes that resemble Popper’s theory of scientific discovery, it misses the mark on doing truly meaningful science.

## Conclusion
In evaluating the potential for autoregressive LLMs to drive scientific discovery, it becomes clear that while LLMs excel in processing and generating text, they fall short of the critical reasoning required for true scientific innovation. Scientific discovery involves a complex interplay of different types of logical reasoning, including abduction, induction, and deduction, which allows scientists to formulate explanatory hypotheses, conduct experiments, and derive conclusions that move the world closer to the ground truth. These processes are deeply rooted in the ability to think abstractly and creatively, qualities that LLMs currently seem to lack.
There is no doubt that LLMs will augment human scientists, and exploring how we could use LLMs as a research “copilot” such as with generating preliminary hypotheses, finding novel connections in massive datasets, and much more would be an intriguing area of research and product development. However, the vision of LLMs as autonomous scientists remains a distant prospect. The transition from data-driven insights to the profound creative and logical leaps inherent to the nature of scientific discovery requires advancements not just in computational power and training dataset size but in the very architecture of model reasoning. The future of AI in science thus lies not in replacing human researchers but in complementing their work, necessitating a focus on developing AI that can work synergistically with humans to push the boundaries of knowledge.
